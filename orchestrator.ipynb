{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Orchestrator\n",
    "## Clasificador de transacciones fraudulentas\n",
    "### Importacion de librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gather": {
     "logged": 1698526719185
    }
   },
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml import automl, Input, MLClient\n",
    "from pprint import pprint\n",
    "from azure.ai.ml.entities import ResourceConfiguration\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Inicio de sesión\n",
    "Ya que estamos usando el presente notebook desde Azure, podemos acceder a nuestras credenciales con unas cuentas lineas de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1698526719537
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /config.json\n"
     ]
    }
   ],
   "source": [
    "credential = DefaultAzureCredential()\n",
    "ml_client = MLClient.from_config(credential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Creación de directorios\n",
    "Procedemos a crear los directorios donde estarán los datasets tanto para pruebas como para validación de nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1698515548286
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directories already exist.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "directory_for_training = \"training-ml-table\"\n",
    "directory_for_validation = \"validation-ml-table\"\n",
    "\n",
    "try:\n",
    "    os.mkdir(directory_for_training)\n",
    "    os.mkdir(directory_for_validation)  \n",
    "except:\n",
    "    print(\"The directories already exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Distribución de datos\n",
    "Se guardara en los directorios creados los dataset correspondientes siguiendo una distribución de 80% para pruebas y 20% para validación.\n",
    "A continuación crearemos dos dataframes con los datos tanto para pruebas como validación de cada dataset. Es decir, crearemos 4 sub dataset que provienen de los 2 que tenemos conocidos como atípicos y no atípicos.\n",
    "\n",
    "Queremos agregar lo importante que es declarar que no haya indices al momento de crear los csv separados ya que con entrenamientos anteriores, cuando queríamos hacer las inferencias de los modelos ya deployados nos pedía un `Column2` el cual hacia referencia a una columna sin nombre, es decir, al índice guardado en el csv. Por culpa de este pequeño error tuvimos que correr todo el orquestador desde el inicio junto al procesamiento de modelos, endpoint y deploys que conlleva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gather": {
     "logged": 1698515548668
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def save_dataset_for_train_and_validation(file_name, test_size, csv_name):\n",
    "    df_credit_card_atypical = pd.read_csv(file_name, index_col=0)\n",
    "\n",
    "    X = df_credit_card_atypical.iloc[:, :-1 ]\n",
    "    y = df_credit_card_atypical.iloc[:, -1 ]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=0)\n",
    "\n",
    "    creditcard_atypical_train_df = pd.merge(X_train, y_train, left_index=True, right_index=True, how=\"inner\")\n",
    "    creditcard_atypical_validation_df = pd.merge(X_test, y_test, left_index=True, right_index=True, how=\"inner\")\n",
    "\n",
    "    creditcard_atypical_train_df.to_csv(f'./{directory_for_training}/train_data_{csv_name}.csv',index=False)\n",
    "    creditcard_atypical_validation_df.to_csv(f'./{directory_for_validation}/valid_data_{csv_name}.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "También queremos aclarar que ya no haremos un entrenamiento con los datos atípicos ya que los entrenamientos anteriores con los no atípicos nos dieron valores de accuracy que tienden a 1, por lo cual consideramos que es innecesario y de esta manera también ahorramos un poco de recursos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "gather": {
     "logged": 1698515563221
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#save_dataset_for_train_and_validation('creditcard_atypical.csv', 0.2, 'atypical')\n",
    "save_dataset_for_train_and_validation('creditcard_non_atypical.csv', 0.2, 'non_atypical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Creación de MLTables\n",
    "Ahora procederemos a la creación de MLTables para referencias nuestros dataset de entrenamiento y validación para nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "gather": {
     "logged": 1698515565202
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import mltable\n",
    "\n",
    "paths_train = [\n",
    "    {'file': './training-ml-table/train_data_non_atypical.csv'}\n",
    "]\n",
    "paths_validation = [\n",
    "    {'file': './validation-ml-table/valid_data_non_atypical.csv'}\n",
    "]\n",
    "\n",
    "train_table = mltable.from_delimited_files(paths_train)\n",
    "train_table.save('./training-ml-table')\n",
    "\n",
    "train_table = mltable.from_delimited_files(paths_validation)\n",
    "train_table.save('./validation-ml-table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Definición de parámetros del Job\n",
    "Ahora definiremos donde están los MLTables previamente creados para luego, una vez guardados, definir el Job donde usaremos un cluster de la serie D11-15 v2 del tipo `Standard_DS11_v2`\n",
    "#### ¿Por qué escogimos esa computadora?\n",
    "Inicialmente planeábamos usar una `Standard_DS2_v2` ya que es una General purpose y para nosotros eso era sinónimo de mas barato, pero indagando mas sobre las demás opciones y teniendo en cuenta el ejemplo en el repositorio de Azure, buscamos mas sobre las serie D, específicamente el modelo `STANDARD_DS12_V2` el cual nos llamo la atención por su precio y la gran cantidad de memoria Ram que ofrece (Ideal para nosotros teniendo en cuenta que tenemos un dataset grande). Por parte del precio nos dimos cuenta que en realidad cobran por la cantidad de cores, siendo la mas barata la `STANDARD_DS11_V2` por 0.185/hora con 2 cores y 14 GB de Ram, y teniendo en cuenta que nuestro Job correrá un máximo de 30 minutos decidimos que no queríamos gastar mas de 1 dólar en el entrenamiento ya que en las pruebas anteriores ya habíamos acumulado mas de 15 dólares por lo cual teniendo en cuenta que una PC cuesta 0.185 la hora, seria 0.0925 para media hora de uso, por lo tanto, la cantidad de nodos para gastar 1 dólar en media hora seria de:\n",
    "\n",
    "$$cantidad = \\frac{1}{0.0925} \\approx 5 nodos$$\n",
    "\n",
    "En resumen, elegimos el PC `STANDARD_DS11_V2` por su gran capacidad de ram (Ideal para nuestro dataset) y su precio accesible. Se usara un máximo de 5 nodos.\n",
    "\n",
    "También declaramos que la columna objetivo es Class ya que es nuestra variable categórica la cual clasifica que transacción es fraudulenta y cual no.\n",
    "\n",
    "Todo lo previamente declaro podemos verlo como entradas, salidas, tiempo y recursos de la siguiente manera:\n",
    "- **Entrada**. Dataset de entrenamiento (`my_training_data_input`) y de validación (`my_validation_data_input`)\n",
    "- **Salida**. Modelos ordenados por la métrica principal definida `accuracy`\n",
    "- **Tiempo**. Definimos un tiempo máximo de ejecución de 30 minutos en `exp_timeout` \n",
    "- **Recursos**. Cluster con el nombre de 'DS11-v2-cpu-cluster' del CPU del tipo `Standard_DS11v2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "gather": {
     "logged": 1698515565498
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "training_mltable_path = \"./training-ml-table/\"\n",
    "validation_mltable_path = \"./validation-ml-table/\"\n",
    "\n",
    "my_training_data_input = Input(type=AssetTypes.MLTABLE, path=training_mltable_path)\n",
    "my_validation_data_input = Input(type=AssetTypes.MLTABLE, path=validation_mltable_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Aquí definimos el cluster, como dijimos anteriormente, usaremos `STANDARD_DS11_V2` con un tiempo de inactividad de 120 segundos para que en caso de no estar usándose en ese lapso de tiempo, los nodos se marquen como inactivos para dejar de incurrir en costos pasados 2 minutos y 5 instancias como máximos ya que es lo que definimos anteriormente para no tener un costo superior a 1 dólar por la duración maxima de ejecución del Job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "gather": {
     "logged": 1698515565820
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target.\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "\n",
    "compute_name = \"DS11-v2-cpu-cluster\"\n",
    "\n",
    "try:\n",
    "    _ = ml_client.compute.get(compute_name)\n",
    "    print(\"Found existing compute target.\")\n",
    "except ResourceNotFoundError:\n",
    "    print(\"Creating a new compute target...\")\n",
    "    compute_config = AmlCompute(\n",
    "        name=compute_name,\n",
    "        type=\"amlcompute\",\n",
    "        size=\"STANDARD_DS11_V2\",\n",
    "        idle_time_before_scale_down=120,\n",
    "        min_instances=0,\n",
    "        max_instances=5,\n",
    "    )\n",
    "    ml_client.begin_create_or_update(compute_config).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "gather": {
     "logged": 1698515566132
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "exp_name = 'creditcard-fraud-classification'\n",
    "exp_timeout = 30\n",
    "\n",
    "fraud_classification_job = automl.classification(\n",
    "    compute=compute_name,\n",
    "    experiment_name=exp_name,\n",
    "    training_data=my_training_data_input,\n",
    "    validation_data=my_validation_data_input,\n",
    "    target_column_name=\"Class\",\n",
    "    primary_metric=\"accuracy\",\n",
    "    tags={\"classification\": \"2\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Creación del Job\n",
    "Ahora procedemos a ejecutar el Jobs previamente definido y también le iniciamos el seguimiento en la segunda celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "gather": {
     "logged": 1698515581042
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your file exceeds 100 MB. If you experience low speeds, latency, or broken connections, we recommend using the AzCopyv10 tool for this file transfer.\n",
      "\n",
      "Example: azcopy copy '/mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-instance-jupyter/code/Users/carrasco.juan12/training-ml-table' 'https://creditcardml5040471872.blob.core.windows.net/azureml-blobstore-9dcd07be-3c9d-4e45-b0d3-2db95e9e2c49/LocalUpload/0ae6e8cf1cf3d01b37e3772236b07ac5/training-ml-table' \n",
      "\n",
      "See https://docs.microsoft.com/azure/storage/common/storage-use-azcopy-v10 for more information.\n",
      "\u001b[32mUploading training-ml-table (155.05 MBs): 100%|██████████| 155050017/155050017 [00:01<00:00, 93714082.35it/s]\n",
      "\u001b[39m\n",
      "\n",
      "\u001b[32mUploading validation-ml-table (38.77 MBs): 100%|██████████| 38765324/38765324 [00:04<00:00, 8108082.61it/s] \n",
      "\u001b[39m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "returned_job = ml_client.jobs.create_or_update(\n",
    "    fraud_classification_job\n",
    ")  # submit the job to the backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "gather": {
     "logged": 1698522045555
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: khaki_planet_gyy3vkmj7n\n",
      "Web View: https://ml.azure.com/runs/khaki_planet_gyy3vkmj7n?wsid=/subscriptions/9d6f3686-6c64-4aea-8d5a-3cd7cb82619b/resourcegroups/ml-proyecto/workspaces/creditcard-ml\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: khaki_planet_gyy3vkmj7n\n",
      "Web View: https://ml.azure.com/runs/khaki_planet_gyy3vkmj7n?wsid=/subscriptions/9d6f3686-6c64-4aea-8d5a-3cd7cb82619b/resourcegroups/ml-proyecto/workspaces/creditcard-ml\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ml_client.jobs.stream(returned_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Deploy\n",
    "### Guardado del tracking URI\n",
    "Procedemos a guardar el tracking URI de nuestro Job para hacerle seguimiento usando MLflow para de esa manera obtener el mejor modelo obtenido por AutoML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "gather": {
     "logged": 1698522047048
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azureml://eastus.api.azureml.ms/mlflow/v1.0/subscriptions/9d6f3686-6c64-4aea-8d5a-3cd7cb82619b/resourceGroups/ML-proyecto/providers/Microsoft.MachineLearningServices/workspaces/CreditCard-ML\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "MLFLOW_TRACKING_URI = ml_client.workspaces.get(\n",
    "    name=ml_client.workspace_name\n",
    ").mlflow_tracking_uri\n",
    "\n",
    "print(MLFLOW_TRACKING_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Obtener el Job que entreno los modelos usando AutoML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "gather": {
     "logged": 1698522047388
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "gather": {
     "logged": 1698522047677
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking.client import MlflowClient\n",
    "from mlflow.artifacts import download_artifacts\n",
    "\n",
    "# Initialize MLFlow client\n",
    "mlflow_client = MlflowClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Resultados del Job\n",
    "Como podemos observar, al llamar a job y mostrar lo que corrió, podemos notar muchas métricas además de los modelos que usando junto a sus resultados de acuerdo a la métrica principal definida. En nuestro caso, la métrica principal es el accuracy y podemos ver los resultados en el campo campo de `score_000`. Como también se puede notar es demasiada información, por lo cual en la siguiente celda mostraremos mas ordenadamente los modelos probados con su respetiva métrica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "gather": {
     "logged": 1698522048131
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Run: data=<RunData: metrics={'AUC_macro': 0.9992520151166466,\n",
      " 'AUC_micro': 0.9992486184022564,\n",
      " 'AUC_weighted': 0.9992520151166466,\n",
      " 'accuracy': 0.9980947577820739,\n",
      " 'average_precision_score_macro': 0.9988802694638901,\n",
      " 'average_precision_score_micro': 0.9988756021883952,\n",
      " 'average_precision_score_weighted': 0.9988799124439014,\n",
      " 'balanced_accuracy': 0.9980929232495093,\n",
      " 'f1_score_macro': 0.9980947421192317,\n",
      " 'f1_score_micro': 0.9980947577820739,\n",
      " 'f1_score_weighted': 0.9980947473741918,\n",
      " 'log_loss': 0.027649516098839436,\n",
      " 'matthews_correlation': 0.996196730447637,\n",
      " 'norm_macro_recall': 0.9961858464990185,\n",
      " 'precision_score_macro': 0.9981038072575847,\n",
      " 'precision_score_micro': 0.9980947577820739,\n",
      " 'precision_score_weighted': 0.9981019831950062,\n",
      " 'recall_score_macro': 0.9980929232495093,\n",
      " 'recall_score_micro': 0.9980947577820739,\n",
      " 'recall_score_weighted': 0.9980947577820739,\n",
      " 'weighted_accuracy': 0.9980965923112435}, params={}, tags={'automl_best_child_run_id': 'khaki_planet_gyy3vkmj7n_4',\n",
      " 'classification': '2',\n",
      " 'fit_time_000': '3.4273029999999998;63.618507;8.213134;5.22455;21.316507;1.145376;4.554163;0.723985;32.547874',\n",
      " 'iteration_000': '0;1;2;3;4;5;6;7;8',\n",
      " 'mlflow.rootRunId': 'khaki_planet_gyy3vkmj7n',\n",
      " 'mlflow.runName': 'khaki_planet_gyy3vkmj7n',\n",
      " 'mlflow.user': 'Juan Pablo Carrasco Padilla',\n",
      " 'model_explain_run': 'best_run',\n",
      " 'pipeline_id_000': '5dfac790c5c209f98a1da2dc1c7fb76f0397324f;c7af0367625be6ac5c2fecbfc72ed444cb7a2111;799d2168db11fc19b9e1c6c1df62f8981ad39fe9;44b5d2d23120dfcf96a7dc6389a293d17a966b1b;628c2351f7c75d74ad3673f2d39af16d6976da2e;3735210984ea10097d5c91905cf6300ac278cd89;2964988f7520e6167bfe42ba2c42ab4a75f8a68c;5304e705d6f401cb07f85bcf0ca59ac1f84dfcf5;5f21d4801ee2a6073890b89985177d37ef49c646',\n",
      " 'predicted_cost_000': '0;0;0.5;0.5;0.5;0.5;0.5;0.5;0.5',\n",
      " 'run_algorithm_000': 'LightGBM;XGBoostClassifier;ExtremeRandomTrees;XGBoostClassifier;KNN;LightGBM;LogisticRegression;LightGBM;KNN',\n",
      " 'run_preprocessor_000': 'MaxAbsScaler;MaxAbsScaler;MaxAbsScaler;SparseNormalizer;StandardScalerWrapper;MaxAbsScaler;RobustScaler;MaxAbsScaler;StandardScalerWrapper',\n",
      " 'score_000': '0.992266958056653;0.9979826847104312;0.9352778011263344;0.9588131461713036;0.9980947577820739;0.9750357232915862;0.9568892251081038;0.958009955824531;0.9976091078049555',\n",
      " 'training_percent_000': '100;100;100;100;100;100;100;100;100'}>, info=<RunInfo: artifact_uri='azureml://eastus.api.azureml.ms/mlflow/v2.0/subscriptions/9d6f3686-6c64-4aea-8d5a-3cd7cb82619b/resourceGroups/ML-proyecto/providers/Microsoft.MachineLearningServices/workspaces/CreditCard-ML/experiments/24678b5b-8483-4931-9426-6f7c0e629956/runs/khaki_planet_gyy3vkmj7n/artifacts', end_time=1698522031347, experiment_id='24678b5b-8483-4931-9426-6f7c0e629956', lifecycle_stage='active', run_id='khaki_planet_gyy3vkmj7n', run_name='khaki_planet_gyy3vkmj7n', run_uuid='khaki_planet_gyy3vkmj7n', start_time=1698515599778, status='KILLED', user_id='211343fa-b568-4d1e-b998-f364d1bb6d6b'>, inputs=<RunInputs: dataset_inputs=[]>>\n"
     ]
    }
   ],
   "source": [
    "job = ml_client.jobs.get(name=returned_job.name)\n",
    "mlflow_parent_run = mlflow_client.get_run(job.name)\n",
    "print(mlflow_parent_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Modelos probados\n",
    "Ahora podemos visualizar tanto los modelos como los pre procesadores usados, cada uno de ellos están ordenados de tal manera que el primer elemento de la lista de modelos pertenece al primer elemento de la lista de pre procesadores y asi sucesivamente. Es por esta razón que mostramos los datos ordenados en un dataframe.\n",
    "Para entender mejor la recopilación de los datos podemos verlo de la siguiente manera:\n",
    "- Modelos. Esta denotado con la etiqueta de `run_algorithm_000`.\n",
    "- Preprocessor. Esta denotado con la etiqueta de `run_preprocessor_000`.\n",
    "- Métrica principal. Esta denotado con la etiqueta de `score_000`. En nuestro caso, como podemos observar arriba, la métrica primaria fue definida como `accuracy` por lo cual los datos que representa `score_000` es el accuracy de cada modelo.\n",
    "### El mejor modelo\n",
    "Como podemos observar en el dataframe, el modelo que tiene el mayor accuracy es un **KNN** con **StandardScalerWrapper**. Pero debemos notar que existe una manera mas directa de acceder al mejore modelo y a su vez a sus métricas. Eso lo mostraremos en la siguiente celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "gather": {
     "logged": 1698522048539
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Algorithm</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>XGBoostClassifier</td>\n",
       "      <td>ExtremeRandomTrees</td>\n",
       "      <td>XGBoostClassifier</td>\n",
       "      <td>KNN</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>KNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Preprocessor</th>\n",
       "      <td>MaxAbsScaler</td>\n",
       "      <td>MaxAbsScaler</td>\n",
       "      <td>MaxAbsScaler</td>\n",
       "      <td>SparseNormalizer</td>\n",
       "      <td>StandardScalerWrapper</td>\n",
       "      <td>MaxAbsScaler</td>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>MaxAbsScaler</td>\n",
       "      <td>StandardScalerWrapper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.99227</td>\n",
       "      <td>0.99798</td>\n",
       "      <td>0.93528</td>\n",
       "      <td>0.95881</td>\n",
       "      <td>0.99809</td>\n",
       "      <td>0.97504</td>\n",
       "      <td>0.95689</td>\n",
       "      <td>0.95801</td>\n",
       "      <td>0.99761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0                  1                   2  \\\n",
       "Algorithm         LightGBM  XGBoostClassifier  ExtremeRandomTrees   \n",
       "Preprocessor  MaxAbsScaler       MaxAbsScaler        MaxAbsScaler   \n",
       "Accuracy           0.99227            0.99798             0.93528   \n",
       "\n",
       "                              3                      4             5  \\\n",
       "Algorithm     XGBoostClassifier                    KNN      LightGBM   \n",
       "Preprocessor   SparseNormalizer  StandardScalerWrapper  MaxAbsScaler   \n",
       "Accuracy                0.95881                0.99809       0.97504   \n",
       "\n",
       "                               6             7                      8  \n",
       "Algorithm     LogisticRegression      LightGBM                    KNN  \n",
       "Preprocessor        RobustScaler  MaxAbsScaler  StandardScalerWrapper  \n",
       "Accuracy                 0.95689       0.95801                0.99761  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_algorithm_000 = mlflow_parent_run.data.tags.get('run_algorithm_000').split(';')\n",
    "run_preprocessor_000 = mlflow_parent_run.data.tags.get('run_preprocessor_000').split(';')\n",
    "score_000 = mlflow_parent_run.data.tags.get('score_000').split(';')\n",
    "\n",
    "accuracy = []\n",
    "for score in score_000:\n",
    "    accuracy.append(np.round(float(score),5))\n",
    "\n",
    "df = pd.DataFrame(index=['Algorithm', 'Preprocessor', 'Accuracy'], data=[run_algorithm_000,run_preprocessor_000,accuracy])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Obtener el mejor modelo\n",
    "Para lograr esto debemos usar la etiqueta de `automl_best_child_run_id` el cual nos referenciara a las métricas del mejor modelo como podemos observar a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "gather": {
     "logged": 1698522048911
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'average_precision_score_macro': 0.9988802694638901,\n",
       " 'weighted_accuracy': 0.9980965923112435,\n",
       " 'AUC_macro': 0.9992520151166466,\n",
       " 'recall_score_macro': 0.9980929232495093,\n",
       " 'balanced_accuracy': 0.9980929232495093,\n",
       " 'recall_score_weighted': 0.9980947577820739,\n",
       " 'AUC_micro': 0.9992486184022564,\n",
       " 'f1_score_macro': 0.9980947421192317,\n",
       " 'precision_score_micro': 0.9980947577820739,\n",
       " 'f1_score_weighted': 0.9980947473741918,\n",
       " 'precision_score_weighted': 0.9981019831950062,\n",
       " 'AUC_weighted': 0.9992520151166466,\n",
       " 'norm_macro_recall': 0.9961858464990185,\n",
       " 'average_precision_score_weighted': 0.9988799124439014,\n",
       " 'matthews_correlation': 0.996196730447637,\n",
       " 'average_precision_score_micro': 0.9988756021883952,\n",
       " 'recall_score_micro': 0.9980947577820739,\n",
       " 'log_loss': 0.027649516098839436,\n",
       " 'accuracy': 0.9980947577820739,\n",
       " 'precision_score_macro': 0.9981038072575847,\n",
       " 'f1_score_micro': 0.9980947577820739}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_child_run_id = mlflow_parent_run.data.tags[\"automl_best_child_run_id\"]\n",
    "best_run = mlflow_client.get_run(best_child_run_id)\n",
    "best_run.data.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Hiperparametros\n",
    "Ingresado al Job podemos notar que tenemos un parámetro denotado como `params`, lo extraño de esto es que no se tiene ningún otro dato en este apartado sin embargo al usar la interfaz de azure podemos notar que si existen algunos hiperparámetros implementados. Creemos que esto se debe a que nosotros no implementamos hiperparámetros desde código por lo cual no nos los muestra. De todos modos intentamos acceder a los hiperparámetros usando las Id de los modelos generado pero nos encontramos con que no podemos acceder a esos recursos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "gather": {
     "logged": 1698522049405
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow_parent_run.data.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "gather": {
     "logged": 1698522114866
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "khaki_planet_gyy3vkmj7n\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "parent_run_id = mlflow_parent_run.info.run_id\n",
    "print(parent_run_id)\n",
    "\n",
    "child_runs = mlflow_client.search_runs(\n",
    "    experiment_ids=[parent_run_id],\n",
    "    filter_string=f\"tags.mlflow.parentRunId = '{parent_run_id}'\"\n",
    ")\n",
    "print(child_runs)\n",
    "for child_run in child_runs:\n",
    "    print(child_run)\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Otro metodo\n",
    "Luego de indagar un poco sobre como funciona el tema de los padres y los hijos con AutoML, pudimos notar que el acceso a los hijos esta bastante limitado según el siguiente [Issue](http://www.limni.net) donde una persona muestra como se pueden acceder a los hijos agregando un numero luego del id del padre, es decir, si el siguiente parent_id definido como `coral_sock_t10p5v595k` entonces para acceder al primer hijo podemos hacer `coral_sock_t10p5v595k_0`. En el siguiente lo demostramos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "gather": {
     "logged": 1698522194756
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hijo #1: affable_pasta_zdzt48kr       \tparams: {}\n",
      "Hijo #2: ashy_pot_wkv4mlkj       \tparams: {}\n",
      "Hijo #3: gifted_pillow_2nbm4cpc       \tparams: {}\n",
      "Hijo #4: great_nerve_1yqf8xbp       \tparams: {}\n",
      "Hijo #5: ashy_pocket_xb3k1z20       \tparams: {}\n",
      "Hijo #6: amusing_cheese_598p8cd9       \tparams: {}\n",
      "Hijo #7: red_mango_clpvc2zl       \tparams: {}\n",
      "Hijo #8: magenta_date_fzym2nvb       \tparams: {}\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "while True:\n",
    "    child_run = mlflow_client.get_run(f'{job.name}_{i}')\n",
    "    if child_run.data.metrics == {}:\n",
    "        break\n",
    "    ml_flow_child_name = child_run.data.tags.get('mlflow.runName')\n",
    "    print(f'Hijo #{i}: {ml_flow_child_name}       \\tparams: {child_run.data.params}')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "De todas formas, aun que tratamos de ver los params definidos para cada hijo, notamos que todos están vacíos y según lo que leímos en el [Issue](http://www.limni.net) anterior, una de las personas llego a la siguiente conclusion:\n",
    "> I believe the issue you're encountering with the data.params dictionary being empty is not related to the Azure ML SDK version. I am guessing this seems to be a limitation of the MLflow API in general.\n",
    "\n",
    "### Descargar del artefacto\n",
    "Ahora procederemos a descargar el artefacto para hacer pruebas de deployment con otros métodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "gather": {
     "logged": 1698522198120
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts downloaded in: /mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-instance-jupyter/code/Users/carrasco.juan12/artifact_downloads/outputs\n",
      "Artifacts: ['conda_env_v_1_0_0.yml', 'engineered_feature_names.json', 'env_dependencies.json', 'featurization_summary.json', 'generated_code', 'mlflow-model', 'model.pkl', 'pipeline_graph.json', 'run_id.txt', 'scoring_file_pbi_v_1_0_0.py', 'scoring_file_v_1_0_0.py', 'scoring_file_v_2_0_0.py']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create local folder\n",
    "local_dir = \"./artifact_downloads\"\n",
    "if not os.path.exists(local_dir):\n",
    "    os.mkdir(local_dir)\n",
    "# Download run's artifacts/outputs\n",
    "local_path = download_artifacts(\n",
    "    run_id=best_run.info.run_id, artifact_path=\"outputs\", dst_path=local_dir\n",
    ")\n",
    "print(\"Artifacts downloaded in: {}\".format(local_path))\n",
    "print(\"Artifacts: {}\".format(os.listdir(local_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "gather": {
     "logged": 1698522198451
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conda.yaml', 'MLmodel', 'model.pkl', 'python_env.yaml', 'requirements.txt']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"./artifact_downloads/outputs/mlflow-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Creación del endpoint\n",
    "Ahora procederemos a crear el endpoint dándole un nombre seguido de la fecha para evitar que existan nombres duplicados ya que de existir nos daría un error en ejecución de nuestro orchestrator. Es importante saber que para nombre existe un limite de 5 caracteres como mínimo y 32 como máximo. Decimos que es importante ya que en nuestra primera ejecución dimos un nombre muy largo lo cual resulto en un error de ejecución, específicamente `BadRequest`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "gather": {
     "logged": 1698522198786
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cc-classification10281943835288\n"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    Model,\n",
    "    Environment,\n",
    "    CodeConfiguration,\n",
    "    ProbeSettings,\n",
    ")\n",
    "\n",
    "# Creating a unique endpoint name with current datetime to avoid conflicts\n",
    "import datetime\n",
    "\n",
    "online_endpoint_name = \"cc-classification\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n",
    "\n",
    "# create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"Endpoint to assess the fraudulence of a transaction\",\n",
    "    auth_mode=\"key\",\n",
    "    tags={\"endpoint\": \"1\"},\n",
    ")\n",
    "print(online_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "gather": {
     "logged": 1698522291669
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ManagedOnlineEndpoint({'public_network_access': 'Enabled', 'provisioning_state': 'Succeeded', 'scoring_uri': 'https://cc-classification10281943835288.eastus.inference.ml.azure.com/score', 'openapi_uri': 'https://cc-classification10281943835288.eastus.inference.ml.azure.com/swagger.json', 'name': 'cc-classification10281943835288', 'description': 'Endpoint to assess the fraudulence of a transaction', 'tags': {'endpoint': '1'}, 'properties': {'azureml.onlineendpointid': '/subscriptions/9d6f3686-6c64-4aea-8d5a-3cd7cb82619b/resourcegroups/ml-proyecto/providers/microsoft.machinelearningservices/workspaces/creditcard-ml/onlineendpoints/cc-classification10281943835288', 'AzureAsyncOperationUri': 'https://management.azure.com/subscriptions/9d6f3686-6c64-4aea-8d5a-3cd7cb82619b/providers/Microsoft.MachineLearningServices/locations/eastus/mfeOperationsStatus/oe:9dcd07be-3c9d-4e45-b0d3-2db95e9e2c49:4965ae3f-a601-42a2-a1f2-2fd18c318c36?api-version=2022-02-01-preview'}, 'print_as_yaml': True, 'id': '/subscriptions/9d6f3686-6c64-4aea-8d5a-3cd7cb82619b/resourceGroups/ml-proyecto/providers/Microsoft.MachineLearningServices/workspaces/creditcard-ml/onlineEndpoints/cc-classification10281943835288', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-instance-jupyter/code/Users/carrasco.juan12', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f0c8227bcd0>, 'auth_mode': 'key', 'location': 'eastus', 'identity': <azure.ai.ml.entities._credentials.IdentityConfiguration object at 0x7f0c82279cf0>, 'traffic': {}, 'mirror_traffic': {}, 'kind': 'Managed'})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Registro del modelo\n",
    "Ahora debemos registrar el modelo para hacer deployment. Es por esta razón que usaremos el mejor modelo mandando la id para poder tener el path adecuado asi como también otros datos como el nombre del modelo `model_name`, la descripción `description` y el tipo `type=AssetTypes.MLFLOW_MODEL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "gather": {
     "logged": 1698522293705
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"cc-clasification-model\"\n",
    "model = Model(\n",
    "    path=f\"azureml://jobs/{best_run.info.run_id}/outputs/artifacts/outputs/mlflow-model/\",\n",
    "    name=model_name,\n",
    "    description=\"Fraudulent transactions classification model\",\n",
    "    type=AssetTypes.MLFLOW_MODEL,\n",
    ")\n",
    "registered_model = ml_client.models.create_or_update(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Deployment\n",
    "Ahora que tenemos tanto el endpoint definido como el modelo registrado, procedemos al deployment ambos definiendo a su vez la instancia sobre la cual estará alojada. Este deploy a su vez también necesita un nombre `name`, el endpoint, el modelo registrado (ambos ya definidos arriba), la instancia `Standard_DS2_v2` la cual tiene un tamaño relativo **X-Small**, elegimos esta instancia ya que consideramos que el costo por tener una instancia funcionando todo tiempo para nuestro deploy puede llevar a costos mas elevados que tenemos a demás de que al ser esta una practica en la cual solo los integrantes del grupo interactuaremos con el endpoint resultante, no necesitaremos mayor fuerza de procesamiento. Por otro lado, al ejecutar nos sale un aviso de que la instancia es muy pequeña, consideramos que si es relevante este aviso pero en un caso donde si existan clientes reales que accedan al endpoint resultante, al tener unicamente 2 clientes (los integrantes del grupo) consideramos que no hay de que preocuparnos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "gather": {
     "logged": 1698522850234
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Instance type Standard_DS2_v2 may be too small for compute resources. Minimum recommended compute SKU is Standard_DS3_v2 for general purpose endpoints. Learn more about SKUs here: https://learn.microsoft.com/en-us/azure/machine-learning/referencemanaged-online-endpoints-vm-sku-list\n",
      "Check: endpoint cc-classification10281943835288 exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........................................................................................................."
     ]
    },
    {
     "data": {
      "text/plain": [
       "ManagedOnlineDeployment({'private_network_connection': None, 'provisioning_state': 'Succeeded', 'endpoint_name': 'cc-classification10281943835288', 'type': 'Managed', 'name': 'cc-clasification-deploy', 'description': None, 'tags': {}, 'properties': {'AzureAsyncOperationUri': 'https://management.azure.com/subscriptions/9d6f3686-6c64-4aea-8d5a-3cd7cb82619b/providers/Microsoft.MachineLearningServices/locations/eastus/mfeOperationsStatus/od:9dcd07be-3c9d-4e45-b0d3-2db95e9e2c49:2ee057d5-c312-46f7-a0a4-4a4eeeb38481?api-version=2023-04-01-preview'}, 'print_as_yaml': True, 'id': '/subscriptions/9d6f3686-6c64-4aea-8d5a-3cd7cb82619b/resourceGroups/ml-proyecto/providers/Microsoft.MachineLearningServices/workspaces/creditcard-ml/onlineEndpoints/cc-classification10281943835288/deployments/cc-clasification-deploy', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/compute-instance-jupyter/code/Users/carrasco.juan12', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f0c9c28ba90>, 'model': '/subscriptions/9d6f3686-6c64-4aea-8d5a-3cd7cb82619b/resourceGroups/ml-proyecto/providers/Microsoft.MachineLearningServices/workspaces/creditcard-ml/models/cc-clasification-model/versions/3', 'code_configuration': None, 'environment': None, 'environment_variables': {}, 'app_insights_enabled': False, 'scale_settings': <azure.ai.ml.entities._deployment.scale_settings.DefaultScaleSettings object at 0x7f0c8227b9a0>, 'request_settings': <azure.ai.ml.entities._deployment.deployment_settings.OnlineRequestSettings object at 0x7f0c8227af80>, 'liveness_probe': <azure.ai.ml.entities._deployment.deployment_settings.ProbeSettings object at 0x7f0c8227b850>, 'readiness_probe': <azure.ai.ml.entities._deployment.deployment_settings.ProbeSettings object at 0x7f0c82279ab0>, 'instance_count': 1, 'arm_type': 'online_deployment', 'model_mount_path': None, 'instance_type': 'Standard_DS2_v2', 'data_collector': None, 'egress_public_network_access': 'Enabled'})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deployment = ManagedOnlineDeployment(\n",
    "    name=\"cc-clasification-deploy\",\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=registered_model.id,\n",
    "    instance_type=\"Standard_DS2_v2\",\n",
    "    instance_count=1)\n",
    "ml_client.online_deployments.begin_create_or_update(deployment).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Inferencias\n",
    "Ahora que tenemos el endpoint levantado y listo para ser usado, cargamos el dataset del mismo tipo (no atípicos) y le sacamos una muestra fraccionaria del 0.00001 lo cual nos da 5 ejemplos. Una vez que tenemos esos ejemplos procedemos a separar en los datos de entrada `df_data` y los resultados esperados `list_result`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "gather": {
     "logged": 1698526758753
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample shape data for testing the endpoint: (5, 10)\n",
      "Columns: Index(['V14', 'V12', 'V3', 'V10', 'V9', 'V16', 'V1', 'V11', 'V4', 'Class'], dtype='object')\n",
      "Data shape: (5, 9) Expected result shape: 5\n"
     ]
    }
   ],
   "source": [
    "df_creditcard = pd.read_csv('creditcard_non_atypical.csv', index_col=0)\n",
    "df_sample_data = df_creditcard.sample(frac=0.00001, replace=True, random_state=1)\n",
    "print(f'Sample shape data for testing the endpoint: {df_sample_data.shape}')\n",
    "print(f'Columns: {df_creditcard.columns}')\n",
    "\n",
    "df_data = df_sample_data.iloc[:,:-1]\n",
    "list_result = df_sample_data.iloc[:,-1].to_list()\n",
    "print(f'Data shape: {df_data.shape} Expected result shape: {len(list_result)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Ahora que tenemos los datos separados, ingresamos a la información de nuestro endpoint para recopilar el código base para realizar las inferencias. Dicho código fue parcialmente editado para trabajar con nuestros datos propuestos donde 'data' sera cada fila de los datos de entrada que recopilamos anteriormente y están en formato pandas.dataframe.\n",
    "Nota: Leyendo la [documentacion](https://learn.microsoft.com/es-es/azure/machine-learning/how-to-deploy-advanced-entry-script?view=azureml-api-1) nos dimos cuenta que el endpoint admite entradas del tipo:\n",
    "- pandas\n",
    "- numpy\n",
    "- pyspark\n",
    "- Objeto estándar de Python\n",
    "\n",
    "Sin embargo, decidimos que haremos las inferencias fila por fila para notar también el tiempo de respuesta entre cada llamada.\n",
    "\n",
    "#### Inferencias usando requests de python\n",
    "A continuación cargaremos el código necesario separado en funciones tanto para pasar la verificación del lado del cliente, formatear nuestras entradas, crear el request, enviar el request y recibirlo para luego mostrarlo en contraste con las entradas esperadas anteriormente recabadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "gather": {
     "logged": 1698526759065
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import os\n",
    "import ssl\n",
    "\n",
    "def allowSelfSignedHttps(allowed):\n",
    "    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "allowSelfSignedHttps(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "gather": {
     "logged": 1698526838962
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def create_endpoint_data(ep_data):\n",
    "  index = list(range(len(ep_data)))\n",
    "  data =  {\n",
    "    \"input_data\": {\n",
    "      \"columns\": [\n",
    "        \"V14\",\n",
    "        \"V12\",\n",
    "        \"V3\",\n",
    "        \"V10\",\n",
    "        \"V9\",\n",
    "        \"V16\",\n",
    "        \"V1\",\n",
    "        \"V11\",\n",
    "        \"V4\"\n",
    "      ],\n",
    "      \"data\": [ep_data]\n",
    "    }\n",
    "  }\n",
    "  return data\n",
    "\n",
    "def create_request(data):\n",
    "  body = str.encode(json.dumps(data))\n",
    "  url = f'https://cc-classification10281943835288.eastus.inference.ml.azure.com/score'\n",
    "  api_key = 'fKpR8WIthnCVFNY5sxeyw04ScPF6Gh2n'\n",
    "  if not api_key:\n",
    "    raise Exception(\"A key should be provided to invoke the endpoint\")\n",
    "\n",
    "  headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key), 'azureml-model-deployment': 'cc-clasification-deploy' }\n",
    "  req = urllib.request.Request(url, body, headers)\n",
    "  return req\n",
    "\n",
    "def make_request(req):\n",
    "  try:\n",
    "    response = urllib.request.urlopen(req)\n",
    "\n",
    "    result = response.read()\n",
    "    return result\n",
    "  except urllib.error.HTTPError as error:\n",
    "    print(\"The request failed with status code: \" + str(error.code))\n",
    "\n",
    "    print(error.info())\n",
    "    print(error.read().decode(\"utf8\", 'ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "gather": {
     "logged": 1698526839356
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for row 136561 is b'[0]' and the real value is 0\n",
      "Prediction for row 522371 is b'[1]' and the real value is 1\n",
      "Prediction for row 500324 is b'[1]' and the real value is 1\n",
      "Prediction for row 521851 is b'[1]' and the real value is 1\n",
      "Prediction for row 394971 is b'[1]' and the real value is 1\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for index, row in df_data.iterrows():\n",
    "    endpoint_data = create_endpoint_data(row.to_list())\n",
    "    request = create_request(endpoint_data)\n",
    "    result = make_request(request)\n",
    "    print(f'Prediction for row {index} is {result} and the real value is {list_result[i]}')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Como podemos observar, todas las inferencias tuvieron una precisión del 100% en este caso, lo cual tiene sentido teniendo en cuenta que tenemos un accuracy muy cercano al 100%.\n",
    "#### Inferencias usando MLClient\n",
    "Ahora haremos las inferencias usando ml_cliente para esto usaremos de nuevo la función que pone en formato nuestras entradas para luego crear el archivo `request file` y una vez que tenemos el archivo el archivo creado procederemos a crear el request con la función `make_ml_request`.\n",
    "\n",
    "Al igual que con la inferencia anterior, tenemos un 100% de exactitud.\n",
    "\n",
    "#### Nota\n",
    "Para esta ultima inferencia se opto por hacer hardcode el nombre del endpoint y del deploy ya que la computadora se nos apago y para volver a tener la información cargada en las variables pertinentes tendrías que volver a cargar todo de nuevo, es decir, volver a crear los endpoint, deployment, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "gather": {
     "logged": 1698526895124
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "request_file_name = 'cc_request_data.json'\n",
    "\n",
    "def create_request_file(data):\n",
    "    with open(request_file_name, 'w') as request_file:\n",
    "        json.dump(data, request_file)\n",
    "\n",
    "def make_ml_request():\n",
    "    resp = ml_client.online_endpoints.invoke(\n",
    "        endpoint_name='cc-classification10281943835288',\n",
    "        deployment_name='cc-clasification-deploy',\n",
    "        request_file=request_file_name\n",
    "    )\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "gather": {
     "logged": 1698526899097
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for row 136561 is [0] and the real value is 0\n",
      "Prediction for row 522371 is [1] and the real value is 1\n",
      "Prediction for row 500324 is [1] and the real value is 1\n",
      "Prediction for row 521851 is [1] and the real value is 1\n",
      "Prediction for row 394971 is [1] and the real value is 1\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for index, row in df_data.iterrows():\n",
    "    endpoint_data = create_endpoint_data(row.to_list())\n",
    "    create_request_file(endpoint_data)\n",
    "    result = make_ml_request()\n",
    "    print(f'Prediction for row {index} is {result} and the real value is {list_result[i]}')\n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "SiGPU",
   "language": "python",
   "name": "sigpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
